{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM Notes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76de931eebb38c37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM Cell\n",
    "![LSTM](/Users/jerrykim/workspace/deep-learning/tensorflow/lstm-input-gate.png)\n",
    "## LSTM Gate Operation\n",
    "![GATE](/Users/jerrykim/workspace/deep-learning/tensorflow/lstm.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e1f8068a6699117"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T03:57:16.924070Z",
     "start_time": "2024-01-17T03:57:16.915100Z"
    }
   },
   "id": "131866a2b54b5065",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e69604d162be992"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T03:57:16.935398Z",
     "start_time": "2024-01-17T03:57:16.926321Z"
    }
   },
   "id": "5781ff62439152c8",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "우리 나라 말이 중국과 달라 한자와는 서로 말이 통하지 아니하여서 이런 까닭으로 어리석은 백성이 말하고자 할 것이 있어도 마침내 제 뜻을 펴지 못하는 사람이 많다. 내가 이것을 가엾게 여겨 새로 스물 여덟 글자를 만드니 모든 사람들로 하여금 쉽게 익혀서 날마다 쓰는 데 편하게 하고자 할 따름이니라.\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T03:57:16.935620Z",
     "start_time": "2024-01-17T03:57:16.930294Z"
    }
   },
   "id": "495a3abd32c2d446",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - x ** 2\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "class MyLSTM(object):\n",
    "    def __init__(self, source, hidden_size, learning_rate=0.01):\n",
    "        self.Y = None\n",
    "        self.I = None\n",
    "        self.F = None\n",
    "        self.O = None\n",
    "        self.C = None\n",
    "        self.CS = None\n",
    "        self.HS = None\n",
    "        self.X = None\n",
    "        self.idx_to_word = None\n",
    "        self.word_to_idx = None\n",
    "        self.vocab = None\n",
    "        self.tokens = None\n",
    "        self.source = source\n",
    "        self.hidden_size = hidden_size        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.data_preprocessing(source)\n",
    "        self.input_size = len(self.vocab) + self.hidden_size # input_size = vocab_size + hidden_size\n",
    "        self.output_size = len(self.vocab) # output_size = vocab_size\n",
    "        \n",
    "        ## weight initialization\n",
    "        ## forget gate\n",
    "        self.W_f = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.b_f = np.zeros((self.hidden_size, 1))\n",
    "        ## input gate\n",
    "        self.W_i = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.b_i = np.zeros((self.hidden_size, 1))\n",
    "        ## cell gate\n",
    "        self.W_c = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.b_c = np.zeros((self.hidden_size, 1))\n",
    "        ## output gate\n",
    "        self.W_o = np.random.randn(self.hidden_size, self.input_size) * 0.1\n",
    "        self.b_o = np.zeros((self.hidden_size, 1))\n",
    "        ## final output\n",
    "        self.W_y = np.random.randn(self.output_size, self.hidden_size) \n",
    "        self.b_y = np.zeros((self.output_size, 1))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.X = {}\n",
    "        self.HS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.CS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.C = {}\n",
    "        self.O = {}\n",
    "        self.F = {}\n",
    "        self.I = {}\n",
    "        self.Y = {}\n",
    "        \n",
    "    def data_preprocessing(self, source):\n",
    "        source = re.sub(r'[^가-힣]', '', source)\n",
    "        self.tokens = data.split()\n",
    "        self.vocab = list(set(self.tokens))\n",
    "        self.word_to_idx = {w:i for i, w in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {i:w for i, w in enumerate(self.vocab)}\n",
    "\n",
    "    def one_hot_encoding(self, idx):\n",
    "        size = len(self.vocab)\n",
    "        one_hot_vector = np.zeros((size, 1))\n",
    "        one_hot_vector[idx][0] = 1\n",
    "        return one_hot_vector\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = {}\n",
    "        outputs = []\n",
    "        for t in range(len(inputs)):\n",
    "            x[t] = self.one_hot_encoding(self.word_to_idx[inputs[t]])\n",
    "            self.X[t] = np.row_stack((x[t], self.HS[t-1]))\n",
    "            self.F[t] = sigmoid(np.dot(self.W_f, self.X[t]) + self.b_f)\n",
    "            self.I[t] = sigmoid(np.dot(self.W_i, self.X[t]) + self.b_i)\n",
    "            self.C[t] = tanh(np.dot(self.W_c, self.X[t]) + self.b_c)\n",
    "            self.O[t] = sigmoid(np.dot(self.W_o, self.X[t]) + self.b_o)\n",
    "            self.CS[t] = self.F[t] * self.CS[t-1] + self.I[t] * self.C[t]\n",
    "            self.HS[t] = self.O[t] * tanh(self.CS[t])\n",
    "            self.Y[t] = softmax(np.dot(self.W_y, self.HS[t]) + self.b_y)\n",
    "            outputs += [np.dot(self.W_y, self.HS[t]) + self.b_y]\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, errors, inputs):\n",
    "        dW_f = np.zeros_like(self.W_f)\n",
    "        dW_i = np.zeros_like(self.W_i)\n",
    "        dW_c = np.zeros_like(self.W_c)\n",
    "        dW_o = np.zeros_like(self.W_o)\n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "        db_c = np.zeros_like(self.b_c)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        dHS_next = np.zeros_like(self.HS[0])\n",
    "        dCS_next = np.zeros_like(self.CS[0])\n",
    "        \n",
    "        for t in reversed(range(len(inputs))):\n",
    "            error = errors[t]\n",
    "            # final Gate            \n",
    "            dW_y += np.dot(error, self.HS[t].T)\n",
    "            db_y += error\n",
    "            \n",
    "            # hidden state error\n",
    "            dHS = np.dot(self.W_y.T, error) + dHS_next\n",
    "            \n",
    "            # output gate error\n",
    "            dO = dHS * tanh(self.CS[t]) * sigmoid_derivative(self.O[t])            \n",
    "            dW_o += np.dot(dO, self.X[t].T)\n",
    "            db_o += dO\n",
    "            \n",
    "            # cell state error\n",
    "            dCS = dHS * self.O[t] * tanh_derivative(self.CS[t]) + dCS_next\n",
    "            \n",
    "            # input gate error\n",
    "            dI = dCS * self.C[t] * sigmoid_derivative(self.I[t])                                   \n",
    "            dW_i += np.dot(dI, self.X[t].T)\n",
    "            db_i += dI\n",
    "            \n",
    "            # cell gate            \n",
    "            dC = dCS * self.I[t] * tanh_derivative(self.C[t])\n",
    "            dW_c += np.dot(dC, self.X[t].T)\n",
    "            db_c += dC\n",
    "            \n",
    "            # forget gate\n",
    "            dF = dCS * self.CS[t-1] * sigmoid_derivative(self.F[t])\n",
    "            dW_f += np.dot(dF, self.X[t].T)\n",
    "            db_f += dF\n",
    "            \n",
    "            # input gate\n",
    "            dZ = np.dot(self.W_f.T, dF) + np.dot(self.W_i.T, dI) + np.dot(self.W_c.T, dC) + np.dot(self.W_o.T, dO)\n",
    "            \n",
    "            # error for next time step\n",
    "            dHS_next = dZ[self.hidden_size:, :]\n",
    "            dCS_next = self.F[t] * dCS\n",
    "        \n",
    "        for d in [dW_f, dW_i, dW_c, dW_o, dW_y, db_f, db_i, db_c, db_o, db_y]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "        \n",
    "        self.W_f -= self.learning_rate * dW_f * (-1)\n",
    "        self.W_i -= self.learning_rate * dW_i * (-1)\n",
    "        self.W_c -= self.learning_rate * dW_c * (-1)\n",
    "        self.W_o -= self.learning_rate * dW_o * (-1)\n",
    "        self.W_y -= self.learning_rate * dW_y * (-1)\n",
    "        self.b_f -= self.learning_rate * db_f * (-1)\n",
    "        self.b_i -= self.learning_rate * db_i * (-1)\n",
    "        self.b_c -= self.learning_rate * db_c * (-1)\n",
    "        self.b_o -= self.learning_rate * db_o * (-1)\n",
    "        self.b_y -= self.learning_rate * db_y * (-1)\n",
    "        \n",
    "    def train(self, epochs, inputs, labels ):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.reset()            \n",
    "            predictions = self.forward(inputs)\n",
    "            errors = []\n",
    "            for t in range(len(predictions)):\n",
    "                error = softmax(predictions[t]) - self.one_hot_encoding(self.word_to_idx[labels[t]])\n",
    "                errors += [error]\n",
    "            self.backward(errors, inputs)\n",
    "        \n",
    "    def test(self, inputs, labels):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward(inputs)\n",
    "        gt = ''\n",
    "        output = '나라의 '\n",
    "        for t in range(len(labels)):\n",
    "            prediction = self.word_to_idx[np.argmax(softmax(probabilities[t].reshape(-1)))]            \n",
    "            gt += inputs[t] + ' '\n",
    "            output += prediction + ' '\n",
    "            if inputs[t] == prediction:\n",
    "                accuracy += 1\n",
    "        print('GT: ', gt)\n",
    "        print('Output: ', output)\n",
    "        \n",
    "    def get_data(self):\n",
    "       return self.tokens[:-1], self.tokens[1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T03:57:16.948931Z",
     "start_time": "2024-01-17T03:57:16.941548Z"
    }
   },
   "id": "1f40c789c64ea085",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (25,1) (44,1) ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m MyLSTM \u001B[38;5;241m=\u001B[39m MyLSTM(data, \u001B[38;5;241m25\u001B[39m, \u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m      2\u001B[0m x_train, y_train \u001B[38;5;241m=\u001B[39m MyLSTM\u001B[38;5;241m.\u001B[39mget_data()\n\u001B[0;32m----> 3\u001B[0m \u001B[43mMyLSTM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m MyLSTM\u001B[38;5;241m.\u001B[39mtest(x_train, y_train)\n",
      "Cell \u001B[0;32mIn[13], line 169\u001B[0m, in \u001B[0;36mMyLSTM.train\u001B[0;34m(self, epochs, inputs, labels)\u001B[0m\n\u001B[1;32m    167\u001B[0m     error \u001B[38;5;241m=\u001B[39m softmax(predictions[t]) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mone_hot_encoding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mword_to_idx[labels[t]])\n\u001B[1;32m    168\u001B[0m     errors \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [error]\n\u001B[0;32m--> 169\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[13], line 115\u001B[0m, in \u001B[0;36mMyLSTM.backward\u001B[0;34m(self, errors, inputs)\u001B[0m\n\u001B[1;32m    112\u001B[0m db_y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m error\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# hidden state error\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m dHS \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW_y\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdHS_next\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;66;03m# output gate error\u001B[39;00m\n\u001B[1;32m    118\u001B[0m dO \u001B[38;5;241m=\u001B[39m dHS \u001B[38;5;241m*\u001B[39m tanh(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mCS[t]) \u001B[38;5;241m*\u001B[39m sigmoid_derivative(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mO[t])            \n",
      "\u001B[0;31mValueError\u001B[0m: operands could not be broadcast together with shapes (25,1) (44,1) "
     ]
    }
   ],
   "source": [
    "MyLSTM = MyLSTM(data, 25, 0.01)\n",
    "x_train, y_train = MyLSTM.get_data()\n",
    "MyLSTM.train(1000, x_train, y_train)\n",
    "MyLSTM.test(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T03:57:17.017911Z",
     "start_time": "2024-01-17T03:57:16.945188Z"
    }
   },
   "id": "36244f9b51731cb3",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-17T03:57:17.009865Z"
    }
   },
   "id": "62703cbfff4fcb1b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
