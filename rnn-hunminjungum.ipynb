{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN implementation in PYTHON"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc3b259dab3d3305"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "data = \"\"\"\n",
    "나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.435981Z",
     "start_time": "2024-01-16T09:13:50.422275Z"
    }
   },
   "id": "initial_id",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    data = re.sub('[^가-힣]', ' ', data)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "    ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_ix, ix_to_word\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.436605Z",
     "start_time": "2024-01-16T09:13:50.433651Z"
    }
   },
   "id": "7272e2b563a5c81c",
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def init_weights(h_size, vocab_size):\n",
    "    U = np.random.randn(h_size, vocab_size) * 0.01\n",
    "    W = np.random.randn(h_size, h_size) * 0.01\n",
    "    V = np.random.randn(vocab_size, h_size) * 0.01\n",
    "    return U,W,V"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.440053Z",
     "start_time": "2024-01-16T09:13:50.438726Z"
    }
   },
   "id": "7d804acf8ca73998",
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def feedforward(inputs, targets, hprev):\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for i in range(seq_len):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][inputs[i]] = 1  # 각각의 word에 대한 one hot coding \n",
    "        hs[i] = np.tanh(np.dot(U, xs[i]) + np.dot(W, hs[i - 1]))\n",
    "        ys[i] = np.dot(V, hs[i])\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i]))  # softmax계산\n",
    "        loss += -np.log(ps[i][targets[i], 0])\n",
    "    return loss, ps, hs, xs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.449462Z",
     "start_time": "2024-01-16T09:13:50.442157Z"
    }
   },
   "id": "55daaf1b2010bc34",
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def backward(ps, hs, xs):\n",
    "\n",
    "    # Backward propagation through time (BPTT)\n",
    "    # 처음에 모든 가중치들은 0으로 설정\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU = np.zeros(U.shape)\n",
    "\n",
    "    for i in range(seq_len)[::-1]:\n",
    "        output = np.zeros((vocab_size, 1))\n",
    "        output[targets[i]] = 1\n",
    "        ps[i] = ps[i] - output.reshape(-1, 1)\n",
    "        # 매번 i스텝에서 dL/dVi를 구하기\n",
    "        dV_step_i = ps[i] @ (hs[i]).T  # (y_hat - y) @ hs.T - for each step\n",
    "\n",
    "        dV = dV + dV_step_i  # dL/dVi를 다 더하기\n",
    "\n",
    "        # 각i별로 V와 W를 구하기 위해서는\n",
    "        # 먼저 공통적으로 계산되는 부분을 delta로 해서 계산해두고\n",
    "        # 그리고 시간을 거슬러 dL/dWij와 dL/dUij를 구한 뒤\n",
    "        # 각각을 합하여 dL/dW와 dL/dU를 구하고 \n",
    "        # 다시 공통적으로 계산되는 delta를 업데이트\n",
    "\n",
    "        # i번째 스텝에서 공통적으로 사용될 delta\n",
    "        delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2)\n",
    "\n",
    "        # 시간을 거슬러 올라가서 dL/dW와 dL/dU를 구하\n",
    "        for j in range(i + 1)[::-1]:\n",
    "            dW_ij = delta_recent @ hs[j - 1].T\n",
    "\n",
    "            dW = dW + dW_ij\n",
    "\n",
    "            dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
    "            dU = dU + dU_ij\n",
    "\n",
    "            # 그리고 다음번 j번째 타임에서 공통적으로 계산할 delta를 업데이트\n",
    "            delta_recent = (W.T @ delta_recent) * (1 - hs[j - 1] ** 2)\n",
    "\n",
    "        for d in [dU, dW, dV]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "    return dU, dW, dV, hs[len(inputs) - 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.449652Z",
     "start_time": "2024-01-16T09:13:50.445880Z"
    }
   },
   "id": "7931229c7b9a985c",
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict(word, length):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[word_to_ix[word]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((h_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
    "        y = np.dot(V, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))    # 소프트맥스\n",
    "        ix = np.argmax(p)                    # 가장 높은 확률의 index를 리턴\n",
    "        x = np.zeros((vocab_size, 1))        # 다음번 input x를 준비\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    pred_words = ' '.join(ix_to_word[i] for i in ixes)\n",
    "    return pred_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.454679Z",
     "start_time": "2024-01-16T09:13:50.449193Z"
    }
   },
   "id": "f5c8f413870a00f6",
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 기본적인 parameters\n",
    "epochs = 10000\n",
    "h_size = 100\n",
    "seq_len = 3\n",
    "learning_rate = 1e-2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.454876Z",
     "start_time": "2024-01-16T09:13:50.450998Z"
    }
   },
   "id": "eeb32836c4505aaf",
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokens, vocab_size, word_to_ix, ix_to_word = data_preprocessing(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.455005Z",
     "start_time": "2024-01-16T09:13:50.452912Z"
    }
   },
   "id": "40cb643c5191ee71",
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{0: '이런',\n 1: '쉬이',\n 2: '아니하기에',\n 3: '못할',\n 4: '달라',\n 5: '스물여덟',\n 6: '만드노니',\n 7: '편안케',\n 8: '문자와',\n 9: '펴지',\n 10: '사람마다',\n 11: '글자를',\n 12: '까닭으로',\n 13: '날로',\n 14: '바가',\n 15: '여겨',\n 16: '백성이',\n 17: '있어도',\n 18: '사람이',\n 19: '가엾이',\n 20: '어리석은',\n 21: '많으니라',\n 22: '익혀',\n 23: '새로',\n 24: '말이',\n 25: '내가',\n 26: '씀에',\n 27: '하여',\n 28: '나라의',\n 29: '이를',\n 30: '통하지',\n 31: '중국과',\n 32: '제',\n 33: '따름이니라',\n 34: '서로',\n 35: '능히',\n 36: '하고자',\n 37: '위해',\n 38: '마침내',\n 39: '할',\n 40: '이르고자',\n 41: '뜻을'}"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:13:50.463933Z",
     "start_time": "2024-01-16T09:13:50.455534Z"
    }
   },
   "id": "bc89de5ee816a979",
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "U, W, V = init_weights(h_size, vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:14:08.738145Z",
     "start_time": "2024-01-16T09:14:08.731491Z"
    }
   },
   "id": "1ab96f5ba5e5d2e1",
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 11.212981572514845\n",
      "epoch 100, loss: 1.6656247872554224\n",
      "epoch 200, loss: 0.26089094182579\n",
      "epoch 300, loss: 0.12941435310994087\n",
      "epoch 400, loss: 0.083728648341404\n",
      "epoch 500, loss: 0.060332210674093945\n",
      "epoch 600, loss: 0.04668973393112946\n",
      "epoch 700, loss: 0.037888070483935\n",
      "epoch 800, loss: 0.03171360352621403\n",
      "epoch 900, loss: 0.027144998131597927\n",
      "epoch 1000, loss: 0.02366621806051932\n",
      "epoch 1100, loss: 0.020960934750283634\n",
      "epoch 1200, loss: 0.018817025177428733\n",
      "epoch 1300, loss: 0.017085778845490927\n",
      "epoch 1400, loss: 0.015660027889379313\n",
      "epoch 1500, loss: 0.014462273955098548\n",
      "epoch 1600, loss: 0.013437359581322642\n",
      "epoch 1700, loss: 0.012546637783123078\n",
      "epoch 1800, loss: 0.011762911380583496\n",
      "epoch 1900, loss: 0.011066571583932537\n",
      "epoch 2000, loss: 0.010443114012686935\n",
      "epoch 2100, loss: 0.009881617884749262\n",
      "epoch 2200, loss: 0.009373675309357135\n",
      "epoch 2300, loss: 0.008912515240743636\n",
      "epoch 2400, loss: 0.008492298833980717\n",
      "epoch 2500, loss: 0.008107651485150793\n",
      "epoch 2600, loss: 0.007753483595996091\n",
      "epoch 2700, loss: 0.0074250754610268605\n",
      "epoch 2800, loss: 0.007118305414524285\n",
      "epoch 2900, loss: 0.00682985724449317\n",
      "epoch 3000, loss: 0.00655728859647672\n",
      "epoch 3100, loss: 0.006298941720108791\n",
      "epoch 3200, loss: 0.006053765686687087\n",
      "epoch 3300, loss: 0.0058211354852043895\n",
      "epoch 3400, loss: 0.005600710199900258\n",
      "epoch 3500, loss: 0.005392339928758322\n",
      "epoch 3600, loss: 0.005196020301217668\n",
      "epoch 3700, loss: 0.005011877793551547\n",
      "epoch 3800, loss: 0.0048401583728194825\n",
      "epoch 3900, loss: 0.004681199695223737\n",
      "epoch 4000, loss: 0.004535383067884806\n",
      "epoch 4100, loss: 0.004403072478462983\n",
      "epoch 4200, loss: 0.004284550886877918\n",
      "epoch 4300, loss: 0.004179961589790155\n",
      "epoch 4400, loss: 0.004089258173500384\n",
      "epoch 4500, loss: 0.004012161920945794\n",
      "epoch 4600, loss: 0.003948120946001107\n",
      "epoch 4700, loss: 0.0038962618386839034\n",
      "epoch 4800, loss: 0.0038553252285046117\n",
      "epoch 4900, loss: 0.003823586087078607\n",
      "epoch 5000, loss: 0.0037987815378923463\n",
      "epoch 5100, loss: 0.0037781012108460714\n",
      "epoch 5200, loss: 0.003758322418826155\n",
      "epoch 5300, loss: 0.003736160312112894\n",
      "epoch 5400, loss: 0.00370881153447973\n",
      "epoch 5500, loss: 0.0036745113377918016\n",
      "epoch 5600, loss: 0.003632824705698328\n",
      "epoch 5700, loss: 0.00358450116428092\n",
      "epoch 5800, loss: 0.003530989605872511\n",
      "epoch 5900, loss: 0.0034743037535391683\n",
      "epoch 6000, loss: 0.003505085391025879\n",
      "epoch 6100, loss: 0.00347254301268154\n",
      "epoch 6200, loss: 0.0034520174467649505\n",
      "epoch 6300, loss: 0.0032650007645922303\n",
      "epoch 6400, loss: 0.003394315312095244\n",
      "epoch 6500, loss: 0.003210105671364696\n",
      "epoch 6600, loss: 0.003281000778840058\n",
      "epoch 6700, loss: 0.0031869432447655583\n",
      "epoch 6800, loss: 0.0030173708145834217\n",
      "epoch 6900, loss: 0.0029598669175234654\n",
      "epoch 7000, loss: 0.002966698603831028\n",
      "epoch 7100, loss: 0.0029118696054383384\n",
      "epoch 7200, loss: 0.0028213885061546783\n",
      "epoch 7300, loss: 0.0027402770884316398\n",
      "epoch 7400, loss: 0.002674388081842088\n",
      "epoch 7500, loss: 0.002610717958918552\n",
      "epoch 7600, loss: 0.0025473293396057793\n",
      "epoch 7700, loss: 0.002492829591381838\n",
      "epoch 7800, loss: 0.0024507327712500722\n",
      "epoch 7900, loss: 0.0024148942592249548\n",
      "epoch 8000, loss: 0.002377931259077152\n",
      "epoch 8100, loss: 0.0023377343487733203\n",
      "epoch 8200, loss: 0.002295479656635322\n",
      "epoch 8300, loss: 0.002252296024095389\n",
      "epoch 8400, loss: 0.0022087541070374135\n",
      "epoch 8500, loss: 0.0021655389087615945\n",
      "epoch 8600, loss: 0.0021235482667412335\n",
      "epoch 8700, loss: 0.0020835229788817335\n",
      "epoch 8800, loss: 0.002045838980489394\n",
      "epoch 8900, loss: 0.002010578348793794\n",
      "epoch 9000, loss: 0.0019776603618004446\n",
      "epoch 9100, loss: 0.0019468960964280934\n",
      "epoch 9200, loss: 0.0019179979941857475\n",
      "epoch 9300, loss: 0.0018906115681899512\n",
      "epoch 9400, loss: 0.001864377232891957\n",
      "epoch 9500, loss: 0.0018389876078165487\n",
      "epoch 9600, loss: 0.0018142150737885405\n",
      "epoch 9700, loss: 0.0017899117439173057\n",
      "epoch 9800, loss: 0.0017659963398870418\n",
      "epoch 9900, loss: 0.0017424384535316977\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "hprev = np.zeros((h_size, 1))\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for p in range(len(tokens)-seq_len):\n",
    "        inputs = [word_to_ix[tok] for tok in tokens[p:p + seq_len]]\n",
    "        targets = [word_to_ix[tok] for tok in tokens[p + 1:p + seq_len + 1]]\n",
    "\n",
    "        loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
    "\n",
    "        dU, dW, dV, hprev = backward(ps, hs, xs)\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        W -= learning_rate * dW\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "\n",
    "        # p += seq_len\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch}, loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:21:07.707615Z",
     "start_time": "2024-01-16T09:14:17.109045Z"
    }
   },
   "id": "5304b7f54c95f80e",
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'말이 중국과 달라 중국과 달라 문자와 달라 문자와 서로 통하지'"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('나라의', 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T09:21:20.396351Z",
     "start_time": "2024-01-16T09:21:20.377311Z"
    }
   },
   "id": "40ee1c3b21d7ae65",
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ac5770846de2d3f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
