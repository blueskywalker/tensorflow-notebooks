{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# My Notes for RNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96fe7dc04c5df515"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T08:43:34.793848Z",
     "start_time": "2024-01-16T08:43:34.786956Z"
    }
   },
   "id": "9ed34c735c23440f",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "data = \"\"\"\n",
    "우리 나라 말이 중국과 달라 한자와는 서로 말이 통하지 아니하여서 이런 까닭으로 어리석은 백성이 말하고자 할 것이 있어도 마침내 제 뜻을 펴지 못하는 사람이 많다. 내가 이것을 가엾게 여겨 새로 스물 여덟 글자를 만드니 모든 사람들로 하여금 쉽게 익혀서 날마다 쓰는 데 편하게 하고자 할 따름이니라.\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T08:43:34.797826Z",
     "start_time": "2024-01-16T08:43:34.793231Z"
    }
   },
   "id": "b8732201e7982399",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Forward Net\n",
    "![forward net](/Users/jerrykim/workspace/deep-learning/tensorflow/forward-net-1150x518.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "442246de7c9d1f69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MyRNN(object):\n",
    "    def __init__(self, source, hidden_dim, seq_length, learning_rate=0.01):\n",
    "        self.word_to_idx = None\n",
    "        self.idx_to_word = None\n",
    "        self.vocab = None\n",
    "        self.tokens = None\n",
    "        self.data_preprocessing(source)\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.input_size = self.output_size = len(self.vocab)        \n",
    "        self.seq_length = seq_length\n",
    "        self.W_xh = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
    "        self.W_hh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
    "        self.b_h = np.zeros((1, self.hidden_size))\n",
    "        self.b_y = np.zeros((1, self.output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def data_preprocessing(self, source):\n",
    "        source = re.sub(r'[^가-힣]', '', source)\n",
    "        self.tokens = data.split()\n",
    "        self.vocab = list(set(self.tokens))        \n",
    "        self.word_to_idx = {w:i for i, w in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {i:w for i, w in enumerate(self.vocab)}\n",
    "\n",
    "\n",
    "    def one_hot_encoding(self, idx):\n",
    "        one_hot_vector = np.zeros((1, self.input_size))\n",
    "        one_hot_vector[0][idx] = 1\n",
    "        return one_hot_vector\n",
    "    \n",
    "    def forward(self, inputs, target, h_prev):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "        loss = 0\n",
    "        for t in range(self.seq_length):\n",
    "            xs[t] = self.one_hot_encoding(inputs[t])            \n",
    "            hs[t] = np.tanh(np.dot(xs[t], self.W_xh) + np.dot(hs[t-1], self.W_hh) + self.b_h)\n",
    "            ys[t] = np.dot(hs[t], self.W_hy) + self.b_y\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            loss += -np.log(ps[t][0][target[t], 0])\n",
    "        return loss, ps, hs, xs\n",
    "\n",
    "    ## backpropagation\n",
    "    def backward(self, ps, hs, xs, target):\n",
    "        dW_xh, dW_hh, dW_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy)\n",
    "        db_h, db_y = np.zeros_like(self.b_h), np.zeros_like(self.b_y)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[0][target[t]] -= 1\n",
    "            dW_hy += np.dot(hs[t].T, dy)\n",
    "            db_y += dy\n",
    "            dh = np.dot(dy, self.W_hy.T) + dh_next\n",
    "            dh_raw = (1 - hs[t] * hs[t]) * dh\n",
    "            db_h += dh_raw\n",
    "            dW_xh += np.dot(xs[t].T, dh_raw)\n",
    "            dW_hh += np.dot(hs[t-1].T, dh_raw)\n",
    "            dh_next = np.dot(dh_raw, self.W_hh.T)\n",
    "        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dW_xh, dW_hh, dW_hy, db_h, db_y\n",
    "\n",
    "    def predict(self, word, length):\n",
    "        h_prev = np.zeros((1, self.hidden_size))\n",
    "        x = self.one_hot_encoding(self.word_to_idx[word])\n",
    "        idxs = []\n",
    "        for t in range(length):\n",
    "            h = np.tanh(np.dot(x, self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h)\n",
    "            y = np.dot(h, self.W_hy) + self.b_y\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            idx = np.random.choice(range(self.input_size), p=p.ravel())\n",
    "            idxs.append(idx)\n",
    "            x = self.one_hot_encoding(idx)\n",
    "            h_prev = h\n",
    "        return ' '.join([self.idx_to_word[i] for i in idxs])\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            h_prev = np.zeros((1, self.hidden_size))\n",
    "            loss = 0\n",
    "            for i in range(len(self.tokens) // self.seq_length):\n",
    "                inputs = [self.word_to_idx[w] for w in self.tokens[i*self.seq_length:(i+1)*self.seq_length]]\n",
    "                target = [self.word_to_idx[w] for w in self.tokens[i*self.seq_length+1:(i+1)*self.seq_length+1]]\n",
    "                loss, ps, hs, xs = self.forward(inputs, target, h_prev)\n",
    "                dW_xh, dW_hh, dW_hy, db_h, db_y = self.backward(ps, hs, xs, target)\n",
    "                for param, dparam in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y], [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n",
    "                    param += -self.learning_rate * dparam\n",
    "                h_prev = hs[self.seq_length-1]\n",
    "            if epoch % 100 == 0:\n",
    "                print('epoch: %d, loss: %f' % (epoch, loss))\n",
    "            \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T08:43:34.817171Z",
     "start_time": "2024-01-16T08:43:34.807154Z"
    }
   },
   "id": "2796ab93e3b92b55",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m rnn \u001B[38;5;241m=\u001B[39m MyRNN(data, \u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mrnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[31], line 69\u001B[0m, in \u001B[0;36mMyRNN.train\u001B[0;34m(self, epochs)\u001B[0m\n\u001B[1;32m     67\u001B[0m inputs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mword_to_idx[w] \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokens[i\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_length:(i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_length]]\n\u001B[1;32m     68\u001B[0m target \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mword_to_idx[w] \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokens[i\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_length\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m:(i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_length\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]]\n\u001B[0;32m---> 69\u001B[0m loss, ps, hs, xs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh_prev\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m dW_xh, dW_hh, dW_hy, db_h, db_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackward(ps, hs, xs, target)\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param, dparam \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_xh, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_hh, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_hy, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb_h, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb_y], [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n",
      "Cell \u001B[0;32mIn[31], line 40\u001B[0m, in \u001B[0;36mMyRNN.forward\u001B[0;34m(self, inputs, target, h_prev)\u001B[0m\n\u001B[1;32m     38\u001B[0m     ys[t] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(hs[t], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_hy) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb_y\n\u001B[1;32m     39\u001B[0m     ps[t] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(ys[t]) \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39mexp(ys[t]))\n\u001B[0;32m---> 40\u001B[0m     loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39mlog(\u001B[43mps\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss, ps, hs, xs\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "rnn = MyRNN(data, 100, 3)\n",
    "rnn.train(2000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T08:43:34.877014Z",
     "start_time": "2024-01-16T08:43:34.815627Z"
    }
   },
   "id": "1886508eb0fd29d3",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-16T08:43:34.876563Z"
    }
   },
   "id": "663b08c7818700c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
