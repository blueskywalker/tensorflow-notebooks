from tqdm import tqdm
import numpy as np
import re

data = """
ìš°ë¦¬ ë‚˜ë¼ ë§ì´ ì¤‘êµ­ê³¼ ë‹¬ë¼ í•œìì™€ëŠ” ì„œë¡œ ë§ì´ í†µí•˜ì§€ ì•„ë‹ˆí•˜ì—¬ì„œ ì´ëŸ° ê¹Œë‹­ìœ¼ë¡œ ì–´ë¦¬ì„ì€ ë°±ì„±ì´ ë§í•˜ê³ ì í•  ê²ƒì´ ìˆì–´ë„ ë§ˆì¹¨ë‚´ ì œ ëœ»ì„ í´ì§€ ëª»í•˜ëŠ” ì‚¬ëŒì´ ë§ë‹¤. ë‚´ê°€ ì´ê²ƒì„ ê°€ì—¾ê²Œ ì—¬ê²¨ ìƒˆë¡œ ìŠ¤ë¬¼ ì—¬ëŸ ê¸€ìë¥¼ ë§Œë“œë‹ˆ ëª¨ë“  ì‚¬ëŒë“¤ë¡œ í•˜ì—¬ê¸ˆ ì‰½ê²Œ ìµí˜€ì„œ ë‚ ë§ˆë‹¤ ì“°ëŠ” ë° í¸í•˜ê²Œ í•˜ê³ ì í•  ë”°ë¦„ì´ë‹ˆë¼."""


# í™œì„±í™” í•¨ìˆ˜
def sigmoid(x):
    return 1 / (1 + np.exp(-x))


def sigmoid_derivative(x):
    return x * (1 - x)


def tanh(x, derivative=False):
    return np.tanh(x)


def tanh_derivative(x):
    return 1 - x ** 2


def softmax(x):
    return np.exp(x) / np.sum(np.exp(x))


def data_preprocessing(src):
    src = re.sub('[^ê°€-í£]', ' ', src)
    t = data.split()
    vocab = list(set(t))
    size = len(vocab)

    word_to_ix = {word: i for i, word in enumerate(vocab)}
    ix_to_word = {i: word for i, word in enumerate(vocab)}

    return t, size, word_to_ix, ix_to_word


tokens, vocab_size, Word_to_ix, ix_to_Word = data_preprocessing(data)


class LSTM:
    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):
        # Hyperparameters
        self.learning_rate = learning_rate
        self.hidden_size = hidden_size
        self.num_epochs = num_epochs

        # Forget Gate
        self.Wf = np.random.randn(hidden_size, input_size)*0.1
        self.bf = np.zeros((hidden_size, 1))

        # Input Gate
        self.Wi = np.random.randn(hidden_size, input_size)*0.1
        self.bi = np.zeros((hidden_size, 1))

        # Candidate Gate
        self.Wc = np.random.randn(hidden_size, input_size)*0.1
        self.bc = np.zeros((hidden_size, 1))

        # Output Gate
        self.Wo = np.random.randn(hidden_size, input_size)*0.1
        self.bo = np.zeros((hidden_size, 1))

        # Final Gate
        self.Wy = np.random.randn(output_size, hidden_size)
        self.by = np.zeros((output_size, 1))

    # ë„¤íŠ¸ì›Œí¬ ë©”ëª¨ë¦¬ ë¦¬ì…‹
    def reset(self):
        self.X = {}

        self.HS = {-1: np.zeros((self.hidden_size, 1))}
        self.CS = {-1: np.zeros((self.hidden_size, 1))}

        self.C = {}
        self.O = {}
        self.F = {}
        self.I = {}
        self.outputs = {}

    # Forward ìˆœì „íŒŒ
    def forward(self, inputs):
        # self.reset()
        x = {}
        outputs = []
        for t in range(len(inputs)):
            x[t] = np.zeros((vocab_size , 1))
            x[t][inputs[t]] = 1  # ê°ê°ì˜ Wordì— ëŒ€í•œ one hot coding
            self.X[t] = np.concatenate((self.HS[t - 1], x[t]))

            self.F[t] = sigmoid(np.dot(self.Wf, self.X[t]) + self.bf)
            self.I[t] = sigmoid(np.dot(self.Wi, self.X[t]) + self.bi)
            self.C[t] = tanh(np.dot(self.Wc, self.X[t]) + self.bc)
            self.O[t] = sigmoid(np.dot(self.Wo, self.X[t]) + self.bo)

            self.CS[t] = self.F[t] * self.CS[t - 1] + self.I[t] * self.C[t]
            self.HS[t] = self.O[t] * tanh(self.CS[t])

            outputs += [np.dot(self.Wy, self.HS[t]) + self.by]

        return outputs

    # ì—­ì „íŒŒ
    def backward(self, errors, inputs):
        dLdWf, dLdbf = 0, 0
        dLdWi, dLdbi = 0, 0
        dLdWc, dLdbc = 0, 0
        dLdWo, dLdbo = 0, 0
        dLdWy, dLdby = 0, 0

        dh_next, dc_next = np.zeros_like(self.HS[0]), np.zeros_like(self.CS[0])
        for t in reversed(range(len(inputs))):
            error = errors[t]

            # Final Gate Weights and Biases Errors
            dLdWy += np.dot(error, self.HS[t].T)         #ğœ•ğ¿/ğœ•ğ‘Šğ‘¦
            dLdby += error                               #ğœ•ğ¿/ğœ•bğ‘¦ = (ğœ•ğ¿/ğœ•z_t)(ğœ•z_t/ğœ•bğ‘¦) = error x 1 (Zt = WyHSt + by)

            # Hidden State Error
            dLdHS = np.dot(self.Wy.T, error) + dh_next    #ğœ•ğ¿/ğœ•ğ»ğ‘†

            # Output Gate Weights and Biases Errors
            dLdo = tanh(self.CS[t]) * dLdHS * sigmoid_derivative(self.O[t])
            dLdWo += np.dot(dLdo, inputs[t].T)
            dLdbo += dLdo

            # Cell State Error
            dLdCS = tanh_derivative(tanh(self.CS[t])) * self.O[t] * dLdHS + dc_next

            # Forget Gate Weights and Biases Errors
            dLdf = dLdCS * self.CS[t - 1] * sigmoid_derivative(self.F[t])
            dLdWf += np.dot(dLdf, inputs[t].T)
            dLdbf += dLdf

            # Input Gate Weights and Biases Errors
            dLdi = dLdCS * self.C[t] * sigmoid_derivative(self.I[t])
            dLdWi += np.dot(dLdi, inputs[t].T)
            dLdbi += dLdi

            # Candidate Gate Weights and Biases Errors
            dLdc = dLdCS * self.I[t] * tanh_derivative(self.C[t])
            dLdWc += np.dot(dLdc, inputs[t].T)
            dLdbc += dLdc

            # Concatenated Input Error (Sum of Error at Each Gate!)
            d_z = np.dot(self.Wf.T, dLdf) + np.dot(self.Wi.T, dLdi) + np.dot(self.Wc.T, dLdc) + np.dot(self.Wo.T, dLdo)

            # Error of Hidden State and Cell State at Next Time Step
            dh_next = d_z[:self.hidden_size, :]
            dc_next = self.F[t] * dLdCS

        for d_ in (dLdWf, dLdbf, dLdWi, dLdbi, dLdWc, dLdbc, dLdWo, dLdbo, dLdWy, dLdby):
            np.clip(d_, -1, 1, out=d_)


        self.Wf += dLdWf * self.learning_rate * (-1)
        self.bf += dLdbf * self.learning_rate * (-1)

        self.Wi += dLdWi * self.learning_rate * (-1)
        self.bi += dLdbi * self.learning_rate * (-1)

        self.Wc += dLdWc * self.learning_rate * (-1)
        self.bc += dLdbc * self.learning_rate * (-1)

        self.Wo += dLdWo * self.learning_rate * (-1)
        self.bo += dLdbo * self.learning_rate * (-1)

        self.Wy += dLdWy * self.learning_rate * (-1)
        self.by += dLdby * self.learning_rate * (-1)

    # Train
    def train(self, inputs, labels):
        for _ in tqdm(range(self.num_epochs)):
            self.reset()
            input_idx = [Word_to_ix[input] for input in inputs]
            predictions = self.forward(input_idx)

            errors = []
            for t in range(len(predictions)):
                errors += [softmax(predictions[t])]
                errors[-1][Word_to_ix[labels[t]]] -= 1

            self.backward(errors, self.X)

    def test(self, inputs, labels):
        accuracy = 0
        probabilities = self.forward([Word_to_ix[input] for input in inputs])

        gt = ''
        output = 'ìš°ë¦¬ '
        for q in range(len(labels)):
            prediction = ix_to_Word[np.argmax(softmax(probabilities[q].reshape(-1)))]
            gt += inputs[q] + ' '
            output += prediction + ' '

            if prediction == labels[q]:
                accuracy += 1

        print('ì‹¤ì œê°’: ', gt)
        print('ì˜ˆì¸¡ê°’: ', output)


def main():
    hidden_size = 25
    lstm = LSTM(input_size=vocab_size+hidden_size, hidden_size=hidden_size, output_size=vocab_size, num_epochs=1000, learning_rate=0.05)
    train_X, train_y = tokens[:-1], tokens[1:]
    lstm.train(train_X, train_y)
    lstm.test(train_X, train_y)


if __name__ == '__main__':
    main()
